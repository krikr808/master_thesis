\chapter{The Finite volume method}
\label{chap:thefinitevolumemethod}

%TODO: Insert time values for more discretized variables
%TODO: Insert error bounds for more discretized equations

%\newrobustcmd{\gammapath}{{\gamma[\vec{r}_1,\,\vec{r}_2]}}
%\newabbrev{\textgammapath}{\mbox{$\gammapath$}}

The \FVM is a way of solving a \PDE, or a set of \PDEs, where the room is \discretized into a large number of non-moving, adjacent volume elements which is commonly referred to as \cells. Different \properties are discretized into certain points. \idxse{scalar}{field}{Scalar fields} are usually discretized to the \idxsp{cell}{center}{s}, or sometimes to \idxp{node}{s} on the \idxsp{cell}{corner}{s}, which can be convenient since \interpolation of fields discretized to the cell centers tend to be more difficult \citep{Losasso2004}. In a \idxs{collocated}{grid}, all properties are stored at the same locations, so the \idxse{vector}{property}{vector properties} are discretized to the same locations as the \idxse{scalar}{property}{scalar properties}. On a \idxs{staggered}{grid} on the other hand, the \velocity (or the \momentum, depending on implementation) is discretized to the \idxsp{cell}{face}{s}. For \thisprojectwork, a staggered grid has been used, and throughout the report the discretization locations for the various properties will be called \idxsp{storage}{location}{s}.

\section{Fluid simulation}

The \FVM can handle \simulation of fluids in a realistic way. It is natural to use this method to represent fluids, since it represents them in a continuous way, and since fluids are continuous medias (at least on the level they are simulated). This can be contrasted to representing the fluids as \idxse{fluid}{particles}{particles}, which are discrete.

When using the \FVM, the simulation can be concentrated to the interesting parts of the flow by using \idxs{adaptive}{mesh refinement} \citep{Popinet2003,Losasso2004}, to speed up the simulation orders of magnitude without loosing orders of magnitude in numerical precision in the interesting parts of the flow. This doesn't really have any natural correspondence when using particles. It is possible to initialize the particles with different \idxsp{effective}{size}{s} in order to make some parts of flow have a lower \idxs{particle}{density} and hence require less \idxs{computational}{power} per unit volume; in that way \idxs{numerical}{precision} can be traded for speed. However, this introduces another problem --- as the flow evolves, \advection may cause large particles to end up at places in the flow where high numerical precision is desired and decrease the numerical precision to under the required level. Besides, if the particles have a high velocity relative to each other (a high \temperature), \diffusion will cause large and small particles to mix and the large particles will once again end up at places in the flow where high numerical precision is desired.

One naive attempt to solve this problem could be to dynamically resize the particles as they end up in parts of the flow with different requirements on the numerical precision. However, this will add or remove mass a those locations, so the simulation will not \idxse{conservation of}{mass}{conserve mass}. A succeeding naive attempt to solve this problem in turn could be to distribute mass that has been removed uniformly over all other particles by scaling them with a factor, but that would lead to a non-physical transportation of mass which would move the center of mass, and it would even cause the simulation to not \idxse{conservation of}{momentum}{conserve momentum}. A better remedy to this problem is to split large particles into smaller ones and merge small particles into larger ones, as first done in a work by \citet{Desbrun1999}, which was later improved upon a number of times, for example by \citet{Yan2009}. However, these techniques still require a fairly large number of particles and are hence not suitable for real time simulations of water surfaces on large bodies of water.

When applying the \FVM in \CFD, it simulates the \flow of a \fluid by dividing the fluid into a large number of non-moving, adjacent \cells and letting the fluid flow between the cells, through the \idxsp{cell}{face}{s}. The motion of the fluid is described by a set of \PDEs, usually the \idxs{Euler}{equations} or the \idxs{Navier--Stokes}{equations}.

The main difference between the Navier--Stokes equations and the Euler equations is that the Navier--Stokes equations takes \index{visousity|see{viscous force}}\idxsp{viscous}{force}{s} into account whe\-reas the Euler equations do not. The Euler equations are therefore a special case of the Navier--Stokes equations. Many textbooks also omit \idxsp{external}{force}{s} when writing about the Euler equations, although gravity which is such an external force usually is included when simulating \idxs{free surface}{flow} using the Euler equations.

In simulations where the viscous force plays a big role, the Euler equations are not sufficient and so the Navier--Stokes equations are usually used, otherwise the Euler equations usually works as well as the Navier--Stokes equations and are even preferred to the Navies--Stokes equations because of the simplification they imply for the model as well as the computations. In \thisprojectwork, it is the Euler equations that are solved.

\section{Divergence calculation}

In the \PDEs, in order to calculate the \divergence of a \idxs{vector}{field}, the \idxs{divergence}{theorem} is used and a \idxs{volume}{integral} of the divergence of the field is converted to a \idxs{surface}{integral} of the vector field itself. The divergence theorem states that
%
\begin{equation} \label{eq:divergence_theorem}
\iiint_V\nabla\cdot\vec{F}(\vec{r'})\,\opd V \,=\, \oiint_S(\vec{F}(\vec{r'})\cdot\normal)\,\opd S
\end{equation}
%
where $\vec{F}$ is a vector field, $V$ is a \idxs{control}{volume}, which in our case is the cell surrounding the point $\vec{r}$ in which the divergence is to be calculated, $S$ is the surface of $V$, with \idxs{normal}{vector} pointing outwards, $\opd V$ and $\opd S$ are \infinitesimal elements in $V$ and $S$ respectively, $\normal$ is the normal of $\opd S$ and $\vec{r'}$ is the position of $\opd V$ and $\opd S$ respectively. The divergence of $\vec{F}(\vec{r})$ is then \approximated as the \average divergence of $\vec{F}$ in $V$ and calculated as
%
\begin{equation} \label{eq:divergence_surface_integral}
\nabla\cdot\vec{F}(\vec{r}) \,=\, \frac{1}{V}\,\oiint_S(\vec{F}\cdot\normal)\,\opd S.
\end{equation}

In the \FVM, the surface of a cell consists of \idxsp{cell}{face}{s}, $S_i$, between the cell itself and \neighboring cells, so \eqref{eq:divergence_surface_integral} can be rewritten as
%
\begin{equation} \label{eq:divergence_cell_face_sum}
\nabla\cdot\vec{F}(\vec{r})\ =\ \frac{1}{V}\,\sum_{S_i} \oiint_{S_i}(\vec{F}\cdot\normal)\,\opd S\ =\ \frac{1}{V}\,\sum_{S_i} F_i\,S_i,
\end{equation}
%
where $i$ is an index, $S_i$ is the \area of the cell face to the $i$th neighbor cell and $F_i$ is the average \idxs{field}{flux} through $S_i$, defined as
%
\begin{equation} \label{eq:fi_integral}
F_i \,=\, \frac{1}{S_i}\oiint_{S_i}(\vec{F}\cdot\normal)\,\opd S.
\end{equation}
%
Besides, cell faces in the \FVM are usually flat, which means that the normal vector $\normal$ is constant for a certain cell face $S_i$. \eqref{eq:fi_integral} can therefore be rewritten as
%
\begin{equation} \label{eq:fi_flat_cell_face}
F_i \,=\, \frac{1}{S_i}\,\normal_i\cdot\oiint_{S_i}\vec{F}\,\opd S,
\end{equation}
%
where $\normal_i$ is the normal of $S_i$. $F_i$, which is now just the $\normal_i$-component of the average value of the field on the cell face $S_i$, is on a staggered grid stored directly on $S_i$.

\section{Gradient calculation}

For \idxsp{orthogonal}{grid}{s}, the \gradient of a \idxs{scalar}{field} is calculated in a similar way, but in this case the \idxs{gradient}{theorem} is used. The gradient theorem states that
%
\begin{equation} \label{eq:gradient_theorem}
\phi(\vec{r}_2)-\phi(\vec{r}_1) \,=\, \int_\gammapath\nabla\phi(\vec{r'})\cdot \opd\vec{r'},
\end{equation}
%
where $\phi$ is a scalar field, \textgammapath is a path within $\phi$'s domain, connecting the vectors $\vec{r}_1$ and $\vec{r}_2$ and $\int_\gammapath$ denotes a \idxs{path}{integral} along \textgammapath. By dividing both sides of \eqref{eq:gradient_theorem} with \mbox{$\Delta r = |\vec{r}_2\,-\,\vec{r}_1|$}, we obtain
%
\begin{equation} \label{eq:gradient_theorem_divided}
\frac{\phi(\vec{r}_2)-\phi(\vec{r}_1)}{\Delta r} \,=\, \frac{\int_\gammapath\nabla\phi(\vec{r'})\cdot \opd\vec{r'}}{\Delta r} \,=\, \frac{\int_\gammapath\nabla\phi(\vec{r'})\cdot\frac{\Delta\vec{r}}{|\Delta\vec{r}|} \opd r'}{\Delta r}
\end{equation}
%
where $\Delta\vec{r} = \vec{r}_2 -  \vec{r}_1$ and
%
\begin{equation}
\nabla\phi(\vec{r})\cdot\frac{\Delta\vec{r}}{|\Delta\vec{r}|} = \phi'_{\Delta\vec{r}}(\vec{r}),
\end{equation}
%
where $\phi'_{\vec{v}}$ is the \derivative of $\phi$ in the direction of $\vec{v}$. By assuming the simplest path possible from $r_1$ to $r_2$, which is just a line segment, $\Delta r$ can be written as
%
\begin{equation}
\Delta r = \int_\gammapath\opd r'
\end{equation}
%
and \eqref{eq:gradient_theorem_divided} becomes
%
\begin{equation} \label{eq:phi_derivative_integral}
\frac{\phi(\vec{r}_2)-\phi(\vec{r}_1)}{\Delta r} \,=\, \frac{\int_\gammapath\phi'_{\Delta\vec{r}}(\vec{r'})\opd r'}{\int_\gammapath\opd r'}
\end{equation}
%
where the right hand side can be identified as the \average value of $\phi'_{\Delta\vec{r}}(\vec{r})$ along the path \textgammapath. Provided that $\vec{r}$ is close enough to \textgammapath (preferably equal to \mbox{$(\vec{r}_1\,+\,\vec{r}_2)/2$}), $\phi'_{\Delta\vec{r}}(\vec{r})$ can be \approximated as this average and calculated as
%
\begin{equation} \label{eq:phi_derivative_final}
\phi'_{\Delta\vec{r}}(\vec{r}) \,=\, \frac{\phi(\vec{r}_2)-\phi(\vec{r}_1)}{\Delta r}.
\end{equation}

The gradient of a scalar field can be written as
%
\begin{equation} \label{eq:gradient_orthogonal}
\nabla\phi(\vec{r}) \,=\, \left(\sum_{i=0}^{d-1}\frac{\partial}{\partial r_i}\,\normvec{e}_i\right) \phi(\vec{r}) \,=\, \sum_{i=0}^{d-1}\phi'_{\normvec{e}_i}(\vec{r})\,\normvec{e}_i,
\end{equation}
%
where $\{\normvec{e}_i\}$ is an \idxs{orthonormal}{basis} for $\mathbb{R}^d$, where $d$ is the number of \dimensions and $i = 0,\,1,\,...\,,d-1$; $\normvec{e}_i$ is a \idxs{base}{vector} in $\{\normvec{e}_i\}$ that is aligned with the $i$th \idxs{grid}{axis} and $r_i$ is the $\normvec{e}_i$ component of $\vec{r}$,
such that $r_i = \normvec{e}_i\cdot\vec{r}$. Since we are on an orthogonal grid, we can assume that the location $\vec{r}$ in which the gradient is to be calculated will be the center (or corner) of a cell with $2d$ neighboring cell centers (or cell corners):
%
\begin{equation} \label{eq:neighboring_locations}
\begin{cases}
\vec{r}_{\normvec{e}_i^-} \,=\, \vec{r} \,-\, \Delta r_i\,\normvec{e}_i\\[1ex]
\vec{r}_{\normvec{e}_i^+} \,=\, \vec{r} \,+\, \Delta r_i\,\normvec{e}_i\\[1ex]
\end{cases}\ ,
\end{equation}
%
where $\Delta r_i$ is the grid spacing in $\normvec{e}_i$-direction. By combining \eqrefs \ref{eq:phi_derivative_final}, \ref{eq:gradient_orthogonal} and \ref{eq:neighboring_locations}, we can write the gradient of $\phi$ as
%
\begin{equation} \label{eq:gradient_final}
\nabla\phi(\vec{r}) \,=\,
\sum_{i=0}^{d-1}\frac{\phi(\vec{r}_{\normvec{e}_i^+})-\phi(\vec{r}_{\normvec{e}_i^-})}{2\,\Delta r_i}\,\normvec{e}_i.
\end{equation}

\section{Navier--Stokes equations}
\label{sec:ns_equations}

The \idxs{Navier--Stokes}{equations} are a statement of the conservation of momentum for a fluid. The \idxse{general form of the}{equations of fluid motion}{general form of the} equations reads
%
\begin{equation} \label{eq:navier_stokes_material_derivative}
\rho\frac{D\vec{u}}{Dt} = -\nabla p + \nabla\cdot\boldsymbol{\mathsf{T}} + \vec{f},
\end{equation}
%
where $\rho$ is the density, $\vec{u}$ is the velocity, $p$ is the pressure, $\devstressten$ is the \index{stress tensor|see{deviatoric stress tensor}}\idxs{deviatoric stress}{tensor}, which includes \idxsp{viscous}{force}{s}, and $\vec{f}$ is the external forces per unit volume. $Dx/Dt$, where $x$ is a scalar or vector field, denotes the material derivative of $x$ and is the time derivative of the property $x$ for a material element following the flow (thus having the velocity $\vec{u}$). The right hand side of this equation is the net force per unit volume acting on the fluid. By multiplying both of the sides with an arbitrary volume, it becomes apparent that the equation is a statement of Newton's second law.

The material derivative of a scalar or vector field $x$ is defined as
%
\begin{equation} \label{eq:material_derivative}
\frac{Dx}{Dt} = \frac{\partial x}{\partial t} + \vec{u}\cdot\nabla x,
\end{equation}
%
and by substituting this in \eqref{eq:navier_stokes_material_derivative}, we obtain
%
\begin{equation} \label{eq:navier_stokes}
\rho\left(\frac{\partial\vec{u}}{\partial t} + \vec{u}\cdot\nabla\vec{u}\right) = -\nabla p + \nabla\cdot\boldsymbol{\mathsf{T}} + \vec{f}.
\end{equation}

For the case of \idxs{Eulerian}{flow}, ${\boldsymbol{\mathsf{T}} = 0}$, so ${\nabla\cdot\boldsymbol{\mathsf{T}}}$ vanishes from the equation. When the right hand side of \eqref{eq:material_derivative} appears in a \PDE, the term $\vec{u}\cdot\nabla x$ is generally referred to as the \emph{advection term} in the \PDE and is responsible for transporting the field with the flow. In \eqref{eq:navier_stokes}, this term is needed in order to give waves their correct speed when the medium is moving.

It should be noted that these equations do not fully describe the behavior of the fluid; for example, they do not model the effects of \idxs{surface}{tension} or describe diffusion of various properties such as \temperature within the fluid, nor do they describe how to obtain any of the fields $p$, $\boldsymbol{\mathsf{T}}$ or $\vec{f}$ that are needed in order to solve the Navier--Stokes equations.

By \idxse{time}{discretization}{time discretizing} and rewriting \eqref{eq:navier_stokes}, and choosing the value of $\vec{u}$ in \timestep $n$ and the value of $\partial\vec{u}/\partial t$ in \timestep $n+\frac{1}{2}$, thus introducing an $O(\Delta t)$ error where $\Delta t$ is the length of the time step, we obtain
%
\begin{equation} \label{eq:navier_stokes_time_discretized}
\vec{u}_{n+1}  = \vec{u}_{n} + \Delta t\left(-\vec{u}_{n}\cdot\nabla\vec{u}_{n} \,+\, \frac{-\nabla p + \nabla\cdot\boldsymbol{\mathsf{T}} + \vec{f}}{\rho}\right),
\end{equation}
%
where $\vec{u}_{n}$ denotes the velocity in time step $n$. Using a method described e.g.\ by \citet{Losasso2004}, \eqref{eq:navier_stokes_time_discretized} can be solved in two steps. First, an auxiliary velocity $\vec{u}^*_n$ that ignores the pressure term is calculated, that is
%
\begin{equation} \label{eq:auxiliary_velocity}
\vec{u}^*_n  = \vec{u}_{n} + \Delta t\left(-\vec{u}_{n}\cdot\nabla\vec{u}_{n} \,+\, \frac{\nabla\cdot\boldsymbol{\mathsf{T}} + \vec{f}}{\rho}\right),
\end{equation}
%
and second, the velocity update is calculated as
%
\begin{equation} \label{eq:velocity_update}
\vec{u}_{n+1} \,=\, \vec{u}^*_n - \Delta t\nabla p_{n+1},
\end{equation}
%
where $p_n$ denotes the pressure in time step $n$.

\section{Continuity equation}

For a \idxs{control}{volume} $V$ with surface $S$ and a surface normal $\normal$ pointing outwards, the amount of \idxs{mass}{flux} $\opd m/\opd t$ entering the control volume can be described by
%
\begin{equation} \label{eq:mass_flux_surface_integral}
\frac{\opd m}{\opd t} \,=\, -\oiint_S(\rho\vec{u}\cdot\normal)\,\opd S,
\end{equation}
%
where $m$ is the mass of the fluid in $V$. By using the \idxs{divergence}{theorem} (\eqref{eq:divergence_theorem}) and dividing with $V$, we can rewrite \eqref{eq:mass_flux_surface_integral} as
%
\begin{equation} \label{eq:mass_flux_volume_integral}
\frac{\opd\,(m/V)}{\opd t} \,=\, -\frac{1}{V}\iiint_V\nabla\cdot(\rho\vec{u})\,\opd V
\end{equation}
%
and in the limit where $V \,\rightarrow\, 0$, this equation turns into
%
\begin{equation} \label{eq:density_partial_time_derivative}
\frac{\partial \rho}{\partial t} \,=\, -\nabla\cdot(\rho\vec{u}),
\end{equation}
%
where the density is defined as $\rho = \opd m/\opd V$. This can be rewritten as
%
\begin{equation} \label{eq:continuity_equation}
\frac{\partial \rho}{\partial t} + \nabla\cdot(\rho\vec{u}) \,=\, 0.
\end{equation}
%
This is known as the \idxs{continuity}{equation} and has to be satisfied in order to ensure \idxs{conservation of}{mass}. \idxse{time}{discretization}{Time discretizing} this equation, and choosing to use the values of $\rho$ and $\vec{u}$ in \timestep $n$ and the value of $\partial \rho/\partial t$ in time step $n+\frac{1}{2}$, thus introducing an $O(\Delta t)$ error, gives
%
\begin{equation} \label{eq:continuity_equation_time_discretized}
\rho_{n+1} \,=\, \rho_{n} - \Delta t\,\nabla\cdot(\rho_{n}\vec{u}_{n}),
\end{equation}
%
where $\rho_n$ denotes the density in \timestep $n$.

\section{Pressure calculation}
\label{sec:pressure_calculation}

Neither the Euler equations nor the Navier--Stokes equations specify how the pressure is updated, so when solving either of these two sets of equations one is essentially free to calculate the pressure in whichever way one desires. Together with a \idxs{pressure}{model}, the two sets of equations come in two major forms which usually differs significantly in implementation and stability: The \indexs{compressible Navier--Stokes}{equations}\indexs{compressible Euler}{equations}\compressible forms and the \indexs{incompressible Navier--Stokes}{equations}\indexs{incompressible Euler}{equations}\incompressible forms. For \thisprojectwork, the compressible Euler equations are solved.

\subsection{Compressible flow}

In nature, all fluids are compressible, so a physically correct \idxs{pressure}{model} will let the fluids contract and expand which means that for \indexs{compressible}{fluid}\idxs{compressible}{flow}, the \divergence of the \idxs{velocity}{field} is allowed to be non-zero. The \pressure is then usually expressed as a function of the density, sometimes also taking into account the \temperature and other \properties that may affect the pressure, that is
%
\begin{equation} \label{eq:pressure_compressible_flow}
p = p\,(\rho,\,T,\,\text{other material properties}),
\end{equation}
%
where $T$ is the temperature.

However, the set of fluid motion equations including \eqref{eq:pressure_compressible_flow} is \idxse{stiff}{equation}{stiff}, meaning that if an ordinary \idxs{explicit}{method} such as the \idxs{forward Euler}{method} is used, the \timestep has to be taken to be extremely small, or the method will be unstable. Ordinary \idxsp{numerical}{method}{s} for solving this set of equations are known to give rise to \idxs{spurious}{oscillations} in the solutions when the \idxs{speed of}{sound} times the \idx{time step} is too large in comparison to the \idxs{characteristic}{length} of the \cells, making the numerical method \unstable. More generally, this stringent restriction of the \timestep is known as the \CFL condition \citep{Courant1967}.

Still, not all solvers for compressible flow suffers from this problem. In a work by \citet{Kwatra2009}, the \CFL condition is alleviated by introducing a \idxs{pressure}{field}, separated from the \idxs{density}{field}, for which a \PDE is derived. In this \PDE, an advection term is then identified, which can be calculated using either a \HO \ENO scheme or semi-Lagrangian advection. The reminding part of the \PDE is then discretized using the \index{implicit Euler method|see{backward Euler method}}\idxs{backward}{Euler method}. This is just the ordinary Euler method but for which the time derivative is calculated in the next time-step instead of in the current. For the pressure field, this time derivative involves the Laplacian of the pressure field in the next time-step, which is not known directly; hence, a \idxs{Poisson}{equation} arises. This technique doesn't lead to spurious acoustic oscillations and is similar to the technique used for solving incompressible flow, which also gives rise to a Poisson equation for solving the pressure field. In the limit where the speed of sound goes to infinity, it leads to the same Poisson equation as for incompressible flow \citep{Kwatra2009}.

\subsection{Incompressible Navier--Stokes equations}

When acoustic waves is of no significant importance to the simulation, it is probably most common to model the fluids as \indexs{incompressible}{fluid}\indexs{incompressible}{flow}\incompressible. When simulating incompressible flow, a different approach is taken to calculate the \idxs{pressure}{field}.

Since the flow is incompressible, the density will be constant, which means that \derivatives of $\rho$ vanishes, that is
%
\begin{equation} \label{eq:density_partial_time_derivative_incompressible_flow}
\frac{\partial \rho}{\partial t} \,=\, 0
\end{equation}
%
and
%
\begin{equation} \label{eq:density_divergence_incompressible_flow}
\nabla\rho \,=\, \vec{0}.
\end{equation}
%
\eqref{eq:continuity_equation} will then turn into
%
\begin{equation} \label{eq:velocity_divergence_incompressible_flow}
\nabla\cdot\vec{u} \,=\, 0.
\end{equation}

By using \eqref{eq:density_divergence_incompressible_flow} and \eqref{eq:velocity_divergence_incompressible_flow}, \eqref{eq:continuity_equation_time_discretized} turns into just
%
\begin{equation} \label{eq:continuity_equation_superfluous}
{\rho_{n+1} \,=\, \rho_n}
\end{equation}
%
and becomes superfluous, since this simplified equation also follows directly from \eqref{eq:density_partial_time_derivative_incompressible_flow}. Furthermore, it turns out that when the flow is incompressible, we have
%
\begin{equation} \label{eq:deviatoric_stress_tensor_incompressible_flow}
\nabla\cdot\boldsymbol{\mathsf{T}} \,=\, \mu\nabla^2\vec{u},
\end{equation}
%
where $\mu$ is the (dynamic) \index{dynamic viscosity|see{viscosity}}\viscosity. For simplicity, we can assume that we use a set of units where $\rho = 1$. \eqref{eq:auxiliary_velocity} can then be rewritten as
%
\begin{equation} \label{eq:auxiliary_velocity_reduced}
\vec{u}^*_n \,= \, \vec{u}_{n} + \Delta t(- \vec{u}_{n}\cdot\nabla\vec{u}_{n} \,+\, \mu\nabla^2\vec{u}_{n} + \vec{f}),
\end{equation}
%
which can be directly solved assuming that $\mu$ and $\vec{f}$ are known.

However, \eqref{eq:velocity_update}, which is used to update the velocity, contains $p$ which is a second unknown and must be calculated before $\vec{u}$ can be calculated. Following standard procedure \citep{Losasso2004}, by rewriting \eqref{eq:velocity_update}, taking the divergence of both sides and using \eqref{eq:velocity_divergence_incompressible_flow} to get rid of $\nabla\cdot\vec{u}$, we obtain the \idxs{Poisson}{equation}
%
\begin{equation} \label{eq:pressure_poisson_equation_incompressible_flow}
\nabla^2 p_{n+1} \,=\, \frac{\nabla\cdot\vec{u}^*_n}{\Delta t}
\end{equation}
%
which needs to be solved before we can update the velocity completely.

When \idxse{spatial}{discretization}{discretizing} this equation spatially a \idxs{system of linear}{equations} is obtained, for which there exist many solution methods with varying speed and accuracy. As a comparison, it can be noted that the naive \idxs{Gaussian}{elimination}\index{algorithm!Gaussian elimination|see{Gaussian elimination}} algorithm, or the \idxs{Gauss--Jordan}{elimination}\index{algorithm!Gauss--Jordan elimination|see{Gauss--Jordan elimination}} algorithm for a \idx{multi-core} system, has a \idxs{time}{complexity} of $O(N^3)$ for an $N\times N$ \idx{matrix}, where the $O$ symbol indicates \idxs{big O}{notation}\index{O!big O notation|see{big O notation}} and $N$ is the number of unknowns. However, an $O(N^3)$ time for solving the pressure Poisson equation would slow down the simulation tremendously, since it otherwise runs in $O(N)$ time per timestep.

Since \incompressibility is only an \approximate \property of the fluid, it is arguably enough to only approximately solve the pressure Poisson equation, which is the governing equation for incompressibility. This assumption enables a large set of fast, iterative methods for solving the pressure equation, such as the \idxse{multilevel}{acceleration}{multilevel accelerated} \idxs{Jacobi}{method} \citep{Popinet2003}. However, there exist iterative methods that will solve the pressure Poisson equation down to \idxs{machine}{precision} in only a few number of iterations, such as the \PCG method, which can be applied if the matrix is \idxse{symmetric}{matrix}{symmetric}; this method has earlier been used with an \idxs{incomplete}{LU Cholesky factorization}\index{factorization!incomplete LU Cholesky|see{incomplete LU Cholesky factorization}}\index{Cholesky factorization!incomplete LU|see{incomplete LU Cholesky factorization}} as \preconditioner \citep{Losasso2004}.

If the pressure equation is only solved approximately, \eqref{eq:velocity_divergence_incompressible_flow} is not perfectly satisfied, and hence \eqref{eq:continuity_equation_superfluous} doesn't hold. If perfect \idxs{conservation of}{mass} is essential, the deviation of mass has to be recorded so that they can be accounted for, and \eqref{eq:continuity_equation_time_discretized} has to be used again.

Instead of trying to satisfy \eqref{eq:velocity_divergence_incompressible_flow}, which obviously doesn't work too well and would lead to uncontrolled fluctuations in density, one would rather prefer that the density very quickly becomes one again. The time $n+1$ density is given by \eqref{eq:continuity_equation_time_discretized} which is fully determined since $\rho_{n+1}$ is the only unknown in the equation, but the time $n+2$ density is given by substituting $n+1$ for $n$ in \eqref{eq:continuity_equation_time_discretized}, that is
%
\begin{equation} \label{eq:continuity_equation_time_discretized_postponed}
\rho_{n+2} \,=\, \rho_{n+1} - \Delta t\,\nabla\cdot(\rho_{n+1}\vec{u}_{n+1})
\end{equation}
%
which is underdetermined since $\vec{u}_{n+1}$ also is unknown. We can therefore make the requirement that
%
\begin{equation} \label{eq:density_conservation}
\rho_{n+2} \,=\, 1.
\end{equation}

Perfect satisfaction of \eqref{eq:density_conservation} will not take place due to an imperfectly solved pressure equation, but is still the goal and also what we are going to assume. By rearranging \eqref{eq:continuity_equation_time_discretized_postponed} and expanding the divergence, we obtain
%
\begin{equation} \label{eq:velocity_divergence_density_conservation_premature}
\nabla\cdot\vec{u}_{n+1} \,=\, \frac{1-\rho_{n+1}^{-1}\,\rho_{n+2}}{\Delta t} \,-\, \rho_{n+1}^{-1}\,\vec{u}_{n+1}\cdot\nabla\rho_{n+1}.
\end{equation}
%
Now, the \idxs{Courant}{number} $C$ is defined as
%
\begin{equation}
C \,=\, \sum_{i=0}^{d-1} \frac{|u_i|\Delta t}{\Delta r_i},
\end{equation}
%
where $u_i$ is the $i$th velocity component, that is $u_i = \normvec{e}_i\cdot\vec{u}$. The Courant number can basically be thought of as the volume fraction of a cell that is replaced during one \timestep because of flow through the cell walls. If we assume that this number is much smaller than one, that is
%
\begin{equation}
C \ll 1,
\end{equation}
%
or if we assume that $C$ is limited and the \spectrum of $\rho_{n+1}$ is dominated by frequencies much lower than the \idxs{Nyquist}{frequency}, $\rho_{n+1}^{-1}\,\vec{u}_{n+1}\cdot\nabla\rho_{n+1}$ will be a minor term in \eqref{eq:velocity_divergence_density_conservation_premature} and can therefore be \approximated as zero. Furthermore, since $\rho_{n+1}$ is close to $1$, we can \approximate $\rho_{n+1}^{-1}$ as the first-order \idxs{Taylor}{polynomial} centered around the value $1$, which is $2-\rho_{n+1}$. Hence, by using \eqref{eq:density_conservation}, we can rewrite \eqref{eq:velocity_divergence_density_conservation_premature} as
%
\begin{equation} \label{eq:velocity_divergence_density_conservation}
\nabla\cdot\vec{u}_{n+1} \,=\, \frac{\rho_{n+1}-1}{\Delta t}.
\end{equation}

By rewriting \eqref{eq:velocity_update}, taking the divergence of both sides and using \eqref{eq:velocity_divergence_density_conservation} to get rid of $\nabla\cdot\vec{u}$, we obtain the \idxs{Poisson}{equation}

\begin{equation} \label{eq:pressure_poisson_equation_density_conservation}
\nabla^2 p_{n+1} \,=\, \frac{\nabla\cdot\vec{u}^*_n}{\Delta t} - \frac{\rho_{n+1} - 1}{\Delta t^2}.
\end{equation}

If, on the other hand the pressure equation is solved to a very high accuracy, or perfect \idxs{conservation of}{mass} is not of especially important, \eqref{eq:pressure_poisson_equation_incompressible_flow} can equally well be used and then the \idxs{density}{field} becomes superfluous. For \simulation purposes, conservation of mass to this high degree is often not important and hence the density field can be omitted.

No matter if we need to conserve mass perfectly or not, we will obtain a \idxs{Poisson}{equation} for the \idxs{pressure}{field}, which can be written on the form
%
\begin{equation} \label{eq:pressure_poisson_equation_continuous}
\nabla^2 p(\vec{r}) \,=\, q(\vec{r}),
\end{equation}
%
where $q$ is a known function of $\vec{r}$.

\subsection{Solution of the pressure Poisson equation}

\label{sec:pressure_poisson_equation_solution}

There are a number of ways to solve the pressure Poisson equation. To begin with, we can realize that since the system is discretized, and since the Poisson equation is linear by nature, the equation can be written as a system of linear equations, that is
%
\begin{equation} \label{eq:pressure_poisson_equation_matrix}
\mathbf{L\,p} \,=\, \mathbf{q},
\end{equation}
%
where $\mathbf{p}$ is the vector containing the pressure in all cells, $\mathbf{q}$ is the vector containing $q(\vec{r})$ for all cells, and $\mathbf{L}$ is a sparse $N\times N$ \idx{matrix} containing the coefficients for the system of linear equations that are obtained when discretizing \eqref{eq:pressure_poisson_equation_continuous}. Since $\mathbf{L}$ is the matrix correspondence of the Laplace operator, $\nabla^2$, found in \eqref{eq:pressure_poisson_equation_continuous}, this particular matrix is also called the Laplacian matrix.

First, there is a class of methods called \idxsp{direct}{method}{s}, which will solve the system of linear equations completely. An example is the naive \idxs{Gaussian}{elimination} algorithm, or the \idxs{Gauss--Jordan}{elimination} algorithm for a \idx{multi-core} system, having a \idxs{time}{complexity} of $O(N^3)$ operations in total in the general case for a system of $N$ unknowns, which is (as already mentioned) unacceptably slow.

Then there is a class of methods called \idxsp{iterative}{method}{s}, which start with an initial, guessed value, $\mathbf{p}^{(0)}$, and then generate a sequence, $\{\mathbf{p}^{(n)}\}$, of improving \approximate solutions, where $n$ is the number of the iterations carried out. Different iterative methods converge at different rates to the real solution, $\mathbf{p}$. For functions that are continuous in time and are used in time-stepping methods to numerically solve differential equations, like the pressure field in the case of \CFD, a value that is often used as an initial guess is the solution of the same equation for the previous time step.

\subsubsection{The Preconditioned Conjugate Gradient Method}

%See also \textit{Incomplete Cholesky Preconditioned Conjugate Gradients method}, described in http://www.cs.ubc.ca/~rbridson/fluidbook/ (Fluid Simulation for Computer Graphics). This method uses the http://en.wikipedia.org/wiki/Incomplete_Cholesky_factorization (incomplete Cholesky factorization) as preconditioner.

One iterative method is the \PCG method, which requires the matrix formulation of the system to be symmetric and positive-definite. This method will reach the exact solution if run for $N$ iterations, and can hence be used as a direct method. On the other hand, it will often produce a very accurate approximation even for a small number of iterations, and is therefore often used as an iterative method and stopped relatively early.

In a work by \citet{Losasso2004}, this method was noted to require only about 20 iterations to converge to an \accuracy of \idxs{machine}{precision} and that the pressure solver that was used only accounted for \mbox{25 \%} of the simulation time. However, it also noted that if the equation formulation is nonsymmetric, it requires nonoptimal preconditioners which easily leads to an order of magnitude slowdown, and in the worst case, even problems with robustly finding a solution at all.
% The \textit{Incomplete Cholesky Preconditioned Conjugate Gradients method} is described in http://www.cs.ubc.ca/~rbridson/fluidbook/ (Fluid Simulation for Computer Graphics).

\subsubsection{The Jacobi Method}

Another iterative method is the \idxs{Jacobi}{method}, where in each iteration, each equation in the system is solved independently of all other equations, by isolating the unknown central to the equation and by replacing the other unknowns with the values obtained for them from the previous iteration.

If $\mathbf{L}$ is decomposed into a diagonal component $\mathbf{D}$ and the reminder $\mathbf{R}$, such that $\mathbf{L} = \mathbf{D} + \mathbf{R}$, this method can be expressed as
%
\begin{equation} \label{eq:jacobi_method}
\mathbf{p}^{(n+1)} \,=\, \mathbf{D}^{-1}(\mathbf{q} - \mathbf{R}\mathbf{p}^{(n)}).
\end{equation}
%
Note that since $\mathbf{D}$ is a diagonal matrix, $\mathbf{D}^{-1}$ will also be a diagonal matrix, where the diagonal elements are the inverses of those in $\mathbf{D}$ at the same positions. By subtracting $\mathbf{p}$ from both sides of \eqref{eq:jacobi_method}, we get
%
\begin{equation}
\renewcommand*{\arraystretch}{1.5}
\begin{array}{c}
\mathbf{p}^{(n+1)} - \mathbf{p} \\
=\, \mathbf{D}^{-1}(\mathbf{q} - \mathbf{D}\,\mathbf{p} - \mathbf{R}\,\mathbf{p}^{(n)}) \,=\, \mathbf{D}^{-1}(\mathbf{q} - (\mathbf{D} + \mathbf{R})\,\mathbf{p} - \mathbf{R}(\mathbf{p}^{(n)} - \mathbf{p})) \\
=\, \mathbf{D}^{-1}(\mathbf{q} - \mathbf{L}\,\mathbf{p} - \mathbf{R}(\mathbf{p}^{(n)} - \mathbf{p})) \,=\, -\mathbf{D}^{-1}\mathbf{R}(\mathbf{p}^{(n)} - \mathbf{p}).
\end{array}
\end{equation}
%
If we define the error of the pressure in each iteration as
%
\begin{equation} \label{eq:pressure_error}
\mathbf{\epsilon}^{(n)} \,=\, \mathbf{p}^{(n)} - \mathbf{p},
\end{equation}
%
we see that
%
\begin{equation} \label{eq:jacobi_method_error}
\mathbf{\epsilon}^{(n+1)} \,=\, -\mathbf{D}^{-1}\mathbf{R}\,\mathbf{\epsilon}^{(n)} \,=\, \mathbf{B}\,\mathbf{\epsilon}^{(n)},
\end{equation}
%
where $\mathbf{B} = -\mathbf{D}^{-1}\mathbf{R}$, and by using induction, we find that
%
\begin{equation} \label{eq:jacobi_method_error_from_initial_error}
\mathbf{\epsilon}^{(n)} \,=\, \mathbf{B}^n\mathbf{\epsilon}^{(0)}.
\end{equation}
%
This tells us that the error will decrease exponentially to the number of iterations. The error can be seen as a linear combination of the \eigenvectors of $\mathbf{B}$, which will decrease with different speeds. In fact, an eigenvector of $\mathbf{B}$ will represent a wave shape, and can be assigned an approximate \wavelength in number of cells. The components in the error that decreases the quickest are the eigenvectors with the corresponding eigenvalues closest to 0.

\eqref{eq:pressure_poisson_equation_matrix} represents a spatial discretization of \eqref{eq:pressure_poisson_equation_continuous}. Performing this discretization involves \approximating the Laplace operator, $\nabla^2$; this approximation is itself discrete and is known as the discrete Laplace operator --- let's denote this $\nabla^2_{\text{D}}$. If we for the moment assume that the grid is a $d$-dimensional, cubic --- or the corresponding for $d \neq 3$ --- grid with the grid spacing $\Delta r$ and with periodic boundaries, the simplest useful definition of $\nabla^2_{\text{D}}$ would be
%
\begin{equation} \label{eq:grid_jacobian_simple}
\renewcommand*{\arraystretch}{2} 
\begin{array}{c}
\displaystyle \nabla^2_{\text{D}}\,\phi(\mathbf{r}) \,=\, \sum_{m=0}^{d-1} \frac{\phi(\mathbf{r}-\Delta r\,\mathbf{e}_m)-2\,\phi(\mathbf{r})+\phi(\mathbf{r}+\Delta r\,\mathbf{e}_m)}{\Delta r^2} \\
\displaystyle =\frac{\,-2\,d\,\phi(\mathbf{r})}{\Delta r^2}+\sum_{m=0}^{d-1} \frac{\phi(\mathbf{r}-\Delta r\,\mathbf{e}_m)+\phi(\mathbf{r}+\Delta r\,\mathbf{e}_m)}{\Delta r^2},
\end{array}
\end{equation}
%
where $\phi(\mathbf{r})$ is an arbitrary scalar or vector field, $\mathbf{r}$ is the location vector, and $\mathbf{e}_m$ is the $m$th orthonormal basis vector. If we use this definition of $\nabla^2_{\text{D}}$ as substitute for $\nabla^2$ when discretizing \eqref{eq:pressure_poisson_equation_continuous}, we obtain
%
\begin{equation} \label{eq:pressure_poisson_equation_discretized}
\frac{-2\,d\,p(\mathbf{r})}{\Delta r^2}+\sum_{m=0}^{d-1} \frac{p(\mathbf{r}-\Delta r\,\mathbf{e}_m)+p(\mathbf{r}+\Delta r\,\mathbf{e}_m)}{\Delta r^2} \,=\, q(\mathbf{r}).
\end{equation}
%
By applying this equation to each point in the grid and comparing the resulting system of linear equations to \eqref{eq:pressure_poisson_equation_matrix}, and by remembering that $\mathbf{D}$ is the diagonal component of $\mathbf{L}$ and that $\mathbf{R}$ is the reminder, we can make the identifications
%
\begin{equation} \label{eq:diagonal_matrix_structured_grid}
(\mathbf{D}\mathbf{p})_j \,=\, \frac{-2\,d\,p(\mathbf{r}_j)}{\Delta r^2} \,=\, \frac{-2\,d\,p_j}{\Delta r^2} \,=\, \left(-\frac{2\,d}{\Delta r^2}\mathbf{p}\right)_j,
\end{equation}
%
and
%
\begin{equation} \label{eq:reminder_matrix_structured_grid}
(\mathbf{R}\mathbf{p})_j \,=\, \sum_{m=0}^{d-1} \frac{p(\mathbf{r}_j-\Delta r\,\mathbf{e}_m)+p(\mathbf{r}_j+\Delta r\,\mathbf{e}_m)}{\Delta r^2},
\end{equation}
%
where $\mathbf{r}_j$ is the location of, and $p_j$ is the pressure in, the $j$th grid point, respectively (note that in this particular case, $\mathbf{D}$ reduces to just a scalar, $-2\,d\,\Delta r^{-2}$, times the identity matrix, $\mathbf{I}$; for a non-structured grid, this is typically not the case). By considering \eqref{eq:diagonal_matrix_structured_grid} for all indexes $j$, we see that
%
\begin{equation}
\mathbf{D}\mathbf{p} \,=\, -\frac{2\,d}{\Delta r^2}\mathbf{p},
\end{equation}
%
and by multiplying both sides in this equation by $-\mathbf{D}^{-1}\Delta r^2/2\,d$, we obtain
%
\begin{equation} \label{eq:inverse_diagonal_matrix_structured_grid}
\mathbf{D}^{-1}\mathbf{p} \,=\, -\frac{\Delta r^2}{2\,d}\mathbf{p}.
\end{equation}

Let's assume that $\mathbf{x}$ is a vector on the form
%
\begin{equation} \label{eq:jacobi_regular_wave}
x_j \,=\, x(\mathbf{r}_j) \,=\, e^{i\mathbf{k}^{\T}\mathbf{r}_j},
\end{equation}
%
where $x_j$ is the $j$th element of $\mathbf{x}$, $i$ is the imaginary unit and $\mathbf{k}$ is a wave vector associated with $\mathbf{x}$. Multiplying $\mathbf{B}$ by this vector and by using \eqref{eq:inverse_diagonal_matrix_structured_grid} yields
%
\begin{equation}
\mathbf{B}\,\mathbf{x} \,=\, -\mathbf{D}^{-1}\mathbf{R}\,\mathbf{x} \,=\, \frac{\Delta r^2}{2\,d}\mathbf{R}\,\mathbf{x}
\end{equation}
%
and by extracting the $j$th element and using \eqref{eq:reminder_matrix_structured_grid} we get
%
\begin{equation}
(\mathbf{B}\,\mathbf{x})_j \,=\, \frac{\Delta r^2}{2\,d}(\mathbf{R}\,\mathbf{x})_j \,=\, \sum_{m=0}^{d-1} \frac{x(\mathbf{r}_j-\Delta r\,\mathbf{e}_m)+x(\mathbf{r}_j+\Delta r\,\mathbf{e}_m)}{2\,d},
\end{equation}
%
which, by using \eqref{eq:jacobi_regular_wave}, can be rewritten as
%
\begin{equation}
(\mathbf{B}\,\mathbf{x})_j \,=\, \sum_{m=0}^{d-1} \frac{e^{-i\mathbf{k}^{\T}\Delta r\,\mathbf{e}_m}+e^{i\mathbf{k}^{\T}\Delta r\,\mathbf{e}_m}}{2\,d}x(\mathbf{r}_j).
\end{equation}
%
Finally, by using Euler's formula and by considering the equation for all indexes $j$, we obtain
%
\begin{equation} \label{eq:jacobian_error_eigenvector_equation_solved}
\mathbf{B}\,\mathbf{x} \,=\ \frac{1}{d}\sum_{m=0}^{d-1} \cos(\mathbf{k}^{\T}\Delta r\,\mathbf{e}_m) \mathbf{x}\, =\, \lambda\mathbf{x},
\end{equation}
%
and we see that $\mathbf{x}$ is an eigenvector of $\mathbf{B}$, with the eigenvalue
%
\begin{equation} \label{eq:jacobian_b_eigenvalue}
\lambda \,=\ \frac{1}{d}\sum_{m=0}^{d-1} \cos(\mathbf{k}^{\T}\Delta r\,\mathbf{e}_m).
\end{equation}

In order for $\mathbf{p}^{(n)}$ to converge to the solution, every eigenvalue $\lambda$ of $\mathbf{B}$ must satisfy the inequality $|\lambda| < 1$. The closer the absolute value of an eigenvalue is to one, the slower the corresponding eigenvector will converge, and the closer an eigenvalue is to 0, the faster the corresponding eigenvector will converge. Eigenvalues whose absolute value is equal to 1 corresponds to eigenvectors that will not converge at all.

Eigenvalues equal to 0 corresponds to eigenvector that will vanish after only one iteration. On a cubic grid, the solution of \eqref{eq:jacobian_b_eigenvalue} shows that there are such components in the error. These are waves with a ratio of wavelength to cell size, $l = 2\,\pi/(|\mathbf{k}|\Delta r)$, ranging between $4/\sqrt{5}$ and $4/\sqrt{3}$, depending on the orientation of the wave vector $\mathbf{k}$.

However, all waves with $l < 4/\sqrt{5}$ will have a negative eigenvalue and \overshoot the \idxs{zero}{vector}. The shorter the wavelength the more the eigenvector will overshoot, and the waves with the shortest wavelength will only converge very slowly. As can bee seen in \eqref{eq:jacobian_error_eigenvector_equation_solved}, the most high-frequent wave will have the eigenvalue $-1$ and \oscillate forth and back between two vectors and not converge at all.

Besides, all waves with $l > 4/\sqrt{3}$ will have positive eigenvalues. As can be seen by analyzing \eqref{eq:jacobian_b_eigenvalue}, the number of iterations needed for a wave to reduce in strength with a certain amount is limited by $\Theta(l^2)\ (l\to\infty)$, where the $\Theta$ symbol indicates big $\Theta$ (theta) notation which in contrast to the big O notation, which only limits upwards, limits both upwards and downwards. This means that the Jacobi method very quickly becomes slow as $l$ becomes large, and becomes extremely slow for grids that consist of more than say 10 or 20 cells across, which most grids do.

In these calculations, the grid has for simplicity been assumed to be cubic (or the corresponding for $d \neq 3$), which is a very ideal case. For an octree grid (see \chapref{chap:octrees}) on the other hand, the cell size is not uniform, which means that \eqref{eq:grid_jacobian_simple} cannot be used to define $\nabla^2_{\text{D}}$, which makes the calculations much more complex. Still, the conclusion made about the slow convergence for high and low values of $l$ still holds in principle even for an octree grid, even though the eigenvectors will look different from waves, which makes it impossible to calculate an exact value for $l$, since $l$ requires the eigenvectors to, locally, have a proper wavelength which the new eigenvectors do not have.

\subsubsection{The Gauss--Seidel Method}

Fortunately, there are remedies for both of the two issues mentioned when discussion the Jacobi Method. The issue with the short wavelengths can be remedied by switching from the Jacobi method to the \idxs{Gauss--Seidel}{method}, which is a variant of the Jacobi method. Just like in the Jacobi method, each equation corresponds to an unknown. But unlike in the Jacobi method, in the Gauss--Seidel method the order in which these equations are solved matter. For each equation that is being solved, the unknowns are replaced by the latest estimations of them, which means that the unknowns that correspond to equations that have already been solved in the current iteration will be replaced by the estimations from the current iteration, while the rest of the unknown, except from the unknown central to the equation that is being solved, are replaced by the estimations from the previous iteration.

One effect of switching from the Jacobi method to the Gauss--Seidel method is that the size of the overshoots for eigenvectors with short wavelengths are reduced significantly, and there will no longer be any short wavelengths converging extremely slowly or not at all.

Also, numerical analysis shows that the speed of the convergence for waves with long wavelengths is increased by almost a factor 2. However, this is just a constant factor, and convergence to a satisfactory level of these wavelengths will still require $\Theta(l^2)\ (l\to\infty)$ iterations which is much too slow for grids that are not very small.

\subsubsection{Chebyshev accelerated Successive Over-Relaxation}

Another technique that can be used to speed up the performance is \SOR \citep{Auzinger2011}, which can be applied to both the Jacobi method and the Gauss--Seidel method. In \SOR, the value that is normally used as the updated value for $\mathbf{p}$ is treated as an intermediate value, and the updated value is instead formed as a linear combination of the old value and this intermediate value. \SOR performs best if the coefficients used in the linear combination are varied between the iterations, and by calculating these coefficients from the \idxsp{Chebyshev}{node}{s}, \SOR can perform a very low rate of convergence for all eigenvectors with eigenvalues within a certain range (or in an ellipse in the complex plane if the eigenvalues are complex, if the coefficients are instead calculated from the poles in a type I Chebyshev filter). However, this range must be located at a distance from 0 (on which side doesn't matter), which means that \SOR still performs poorly for eigenvectors with eigenvalues close to zero, which in this case corresponds to eigenvectors with high values of $l$.

%Since we can't resolve the issue related to the eigenvectors with long wavelengths, we need another way to tackle it.

\subsubsection{The Multigrid Method}

We have concluded that in the Jacobi method (if using \SOR) or the Gauss--Seidel method, eigenvectors with small $l$ in the Jacobi method (if using \SOR) or the Gauss--Seidel method are not a problem, but that eigenvectors with high $l$ are, which needs to be tackled. Can we somehow overcome the $\Theta(l^2)\ (l\to\infty)$ requirement for the number of iterations that affects eigenvectors with large $l$? Unfortunately not. Can we decrease $l$ somehow? Well, $l$ is the ratio of wavelength to cell size. We cannot decrease the wavelength, but it is possible to increase the cell size, simply by coarsening the grid.

In the \idxs{multigrid}{method}, the grid is continuously coarsened until a low enough grid resolution, usually meaning that the grid is just a few cells across, is obtained. When applied to solving the pressure Poisson equation, the pressure field is downsampled into a new discretization each time the grid is coarsened, and a new set of linear equations is created for that discretization. Since the cell size increases each time the grid is coarsened, the convergence of long wavelengths in the pressure field is accelerated as $l$ decreases.

As already mentioned, the grid is continuously coarsened and the pressure field is downsampled until a desired resolution is obtained. Now, starting from the coarsest grid, one or a few iterations with the Gauss--Seidel method or with \SOR are carried out in order to find an \approximate solution of the pressure equation. The difference between the discretized pressure field stored on that level before and after the iterations is upsampled and added to the discretized pressure field stored on the second coarsest grid. A few iterations of the same iterative method are carried out and the difference in the discretized pressure field stored on that grid is upsampled and added to the third coarsest grid, and so on until the finest grid has been reached. Finally, a few iterations of the iterative method used on the other grids are carried out in order to approximately solve the pressure equation even on the finest level.

Because upsampling needs some way to interpolate the discretized pressure field, and often has a slight \idxsp{low-pass}{filter}{ing} effect simply because it tends to smear it out, additional errors are introduced in the pressure field. Hence, the entire process of coarsening and refining the grid usually has to be repeated a number of times before a solution of the desired accuracy can be reached.

Note that although a single grid is unable to quickly make long wavelengths in the pressure field converge with an acceptable speed, the multigrid method makes sure those wavelengths are accounted for in the coarser grids. Besides, thanks to the exponentially decreasing number of grid points in each grid, the total number of grid points that have to be processed can be approximated as a \idxs{geometric}{series} containing the number of grid points in the finest grid, $N$. Considering also that the multigrid method will reduce errors by a constant factor each time the process is repeated, the time complexity of the multigrid method for solving the pressure equation is $O(mN)$, where $m$ is the number of orders of magnitude with which the error is reduced when applying the multigrid method. It is because of the existence of $m$ in this expression \eqref{eq:pressure_poisson_equation_density_conservation} was developed as a replacement for \eqref{eq:pressure_poisson_equation_incompressible_flow}, in order to make the simulation less sensitive to fluctuations in the divergence of the \idxs{velocity}{field} and hence allow the pressure equation to be solved only \approximately.

Finally, although the multigrid method can be applied on an octree \citep{Popinet2003,Ji2012}, the implementation becomes slightly more complicated than on a regular grid. To ensure optimal performance in the general case, a special method that coarsens the grid on multiple levels simultaneously has to be used \citep{Popinet2003}.

%\subsection{Semicompressible water}

%\section{Boundary conditions}